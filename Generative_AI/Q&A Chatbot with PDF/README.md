# Conversational RAG with PDF Uploads and Chat History

This project implements a conversational retrieval-augmented generation (RAG) system where users can upload PDF files and engage in a Q&A session with the content of the PDFs. The chat history is managed, and answers are generated using a language model (LLM) from the Groq API.

## Project Overview
- Technologies used:
  - Langchain for retrieval and question-answering chains.
  - FAISS for vector storage and retrieval.
  - Groq for the LLM.
  - Streamlit for building the user interface.
  - PyPDFLoader for loading and processing PDF files.


## Features
- Upload multiple PDFs and interact with their content.
- History-aware retriever that formulates questions considering the conversation history.
- Display chat history for context-based question-answering.
- Integration with the Groq API for LLM responses.
- Text embeddings using HuggingFace's all-MiniLM-L6-v2 model.

## Prerequisites
- Python 3.10
- Groq API key
- HuggingFace token (optional, if required by HuggingFace embeddings)

## Usage
1 - Enter your Groq API key in the input field provided.
2 - Upload PDF files using the file uploader.
3 - Ask questions related to the content of the PDFs in the chat interface.
4 - View the answers generated by the model along with the chat history.

## Key Functions
- Vector Store (FAISS): Documents are split into chunks and stored as vectors for efficient retrieval using embeddings.
- History-Aware Retrieval Chain: The retriever takes the chat history into account when processing user queries.
- Question Answering: Answers are generated concisely using the retrieved context from the PDF content.

